{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71dcafb",
   "metadata": {},
   "source": [
    "# Notebook 6: Data Preparation for Semantic Clustering\n",
    "\n",
    "**Objective:** Prepare the specific dataset (26 occupations from Bias in Bios) and generate *contextualized* embeddings using GPT-2 based on representative sentences. This involves:\n",
    "1. Loading the list of target occupations and their stereotype labels for clustering.\n",
    "2. Loading the dictionary of sampled sentences for these occupations.\n",
    "3. Filtering out excluded occupations ('psychologist', 'surgeon').\n",
    "4. Loading the GPT-2 model and tokenizer.\n",
    "5. Defining a function to generate contextualized embeddings by averaging sentence embeddings.\n",
    "6. Generating and saving the embeddings and the corresponding metadata (occupation list + labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0fd5b",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1793bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc6a3f5",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3fad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "# Get project root assuming the notebook is in 'notebooks' directory\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a787bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files\n",
    "# Assumes a CSV listing occupations for clustering and their BLS labels\n",
    "CLUSTER_OCCUPATIONS_CSV = project_root / 'data' / 'bib_bias_cluster_analysis.csv'\n",
    "# Pickle file containing dict: {occupation_name: [sentence1, sentence2, ...]}\n",
    "SENTENCES_PICKLE_FILE = project_root / 'data' / 'processed' / 'bib_sampled_sentences.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "838bdfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output files\n",
    "RESULTS_DIR = project_root / 'results'\n",
    "CLUSTER_RESULTS_DIR = RESULTS_DIR / 'semantic_clustering' # Subdirectory for clustering results\n",
    "CLUSTER_METADATA_OUTPUT_CSV = CLUSTER_RESULTS_DIR / 'clustering_metadata.csv'\n",
    "CLUSTER_EMBEDDINGS_OUTPUT_NPZ = CLUSTER_RESULTS_DIR / 'clustering_contextual_embeddings.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b47b8dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Configuration ---\n",
    "MODEL_NAME = 'gpt2'\n",
    "# Occupations mentioned in the paper to be excluded from clustering analysis\n",
    "EXCLUDED_OCCUPATIONS = ['psychologist', 'surgeon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c54c87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if it doesn't exist\n",
    "CLUSTER_RESULTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14551ec",
   "metadata": {},
   "source": [
    "## 3. Load Sentences and Occupation Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e92f2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sentences for 28 occupations from /Users/jessie/Documents/Projects/master_thesis_llms_bias/data/processed/bib_sampled_sentences.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load sentences\n",
    "try:\n",
    "    with open(SENTENCES_PICKLE_FILE, 'rb') as f:\n",
    "        sentences_dict = pickle.load(f)\n",
    "    print(f\"Loaded sentences for {len(sentences_dict)} occupations from {SENTENCES_PICKLE_FILE}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Sentences file not found at {SENTENCES_PICKLE_FILE}\")\n",
    "    print(\"Please ensure the sentence extraction step was completed.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading sentences pickle: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "756e8b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for 28 occupations from /Users/jessie/Documents/Projects/master_thesis_llms_bias/data/bib_bias_cluster_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "# Load occupation list and stereotypes for clustering\n",
    "\n",
    "df_cluster_occupations = pd.read_csv(CLUSTER_OCCUPATIONS_CSV)\n",
    "print(f\"Loaded metadata for {len(df_cluster_occupations)} occupations from {CLUSTER_OCCUPATIONS_CSV}\")\n",
    "# Validate required columns\n",
    "required_cols = ['occupation', 'bls_label'] # Expecting these based on analysis of provided scripts\n",
    "if not all(col in df_cluster_occupations.columns for col in required_cols):\n",
    "     missing = [col for col in required_cols if col not in df_cluster_occupations.columns]\n",
    "     raise ValueError(f\"Missing required columns in {CLUSTER_OCCUPATIONS_CSV}: {missing}. Available: {df_cluster_occupations.columns.tolist()}\")\n",
    "\n",
    "\n",
    "# Clean occupation names\n",
    "df_cluster_occupations['occupation'] = df_cluster_occupations['occupation'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044b07d",
   "metadata": {},
   "source": [
    "## 4. Filter Occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15bdcdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on EXCLUDED_OCCUPATIONS list\n",
    "df_filtered_occupations = df_cluster_occupations[\n",
    "    ~df_cluster_occupations['occupation'].isin(EXCLUDED_OCCUPATIONS)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5203c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on availability in sentences dictionary\n",
    "original_count = len(df_filtered_occupations)\n",
    "df_final_occupations = df_filtered_occupations[\n",
    "    df_filtered_occupations['occupation'].isin(sentences_dict.keys())\n",
    "].reset_index(drop=True)\n",
    "final_count = len(df_final_occupations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1c2308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded 2 specific occupations.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Excluded {len(EXCLUDED_OCCUPATIONS)} specific occupations.\")\n",
    "if final_count < original_count:\n",
    "    print(f\"Excluded {original_count - final_count} additional occupations not found in the sentences dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c1865b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of occupations for clustering: 26\n"
     ]
    }
   ],
   "source": [
    "# Verify the final count (expected to be 26 based on paper)\n",
    "expected_count = 26\n",
    "print(f\"Final number of occupations for clustering: {final_count}\")\n",
    "if final_count != expected_count:\n",
    "    print(f\"Warning: Expected {expected_count} occupations after filtering, but got {final_count}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7375291",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Keep only the sentences for the final list of occupations\n",
    "final_occupation_list = df_final_occupations['occupation'].tolist()\n",
    "final_sentences_dict = {occ: sentences_dict[occ] for occ in final_occupation_list if occ in sentences_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77eeff",
   "metadata": {},
   "source": [
    "## 5. Setup Model and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e97dda0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using MPS (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "# Setup Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print(\"\\nUsing GPU:\", torch.cuda.get_device_name(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "    print(\"\\nUsing MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(\"\\nUsing CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48f9705f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2 tokenizer and model...\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Model (same setup as Notebook 2)\n",
    "print(f\"Loading {MODEL_NAME} tokenizer and model...\")\n",
    "try:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = GPT2Model.from_pretrained(MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c6282",
   "metadata": {},
   "source": [
    "## 6. Define Contextual Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6463d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contextual_embedding(occupation_name, sentences, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Generates a contextualized embedding for an occupation by averaging\n",
    "    the embeddings of its representative sentences. Each sentence embedding\n",
    "    is obtained via masked mean pooling over its tokens.\n",
    "\n",
    "    Args:\n",
    "        occupation_name (str): Name of the occupation (for logging).\n",
    "        sentences (list[str]): List of sentences for the occupation.\n",
    "        tokenizer: The loaded Hugging Face tokenizer.\n",
    "        model: The loaded Hugging Face model.\n",
    "        device: The torch device ('cuda', 'mps', or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray or None: The averaged embedding vector as a NumPy array,\n",
    "                             or None if an error occurs or no valid sentence embeddings generated.\n",
    "    \"\"\"\n",
    "    sentence_embeddings = []\n",
    "    if not sentences:\n",
    "        print(f\"Warning: No sentences provided for occupation '{occupation_name}'. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if not isinstance(sentence, str) or not sentence.strip():\n",
    "            # print(f\"Skipping invalid sentence for {occupation_name}: {sentence}\") # Optional: verbose logging\n",
    "            continue\n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(\n",
    "                sentence,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=tokenizer.model_max_length, # Use model's max length\n",
    "                return_attention_mask=True\n",
    "            ).to(device)\n",
    "\n",
    "            # Get model output\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "            # Masked Mean Pooling for the sentence\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "            masked_embeddings = last_hidden_states * mask_expanded\n",
    "            summed_embeddings = torch.sum(masked_embeddings, 1)\n",
    "            token_counts = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "            mean_pooled_embedding = summed_embeddings / token_counts\n",
    "\n",
    "            embedding_np = mean_pooled_embedding.squeeze().cpu().numpy()\n",
    "\n",
    "            if not np.isnan(embedding_np).any():\n",
    "                sentence_embeddings.append(embedding_np)\n",
    "            # else: # Optional: Log if a sentence embedding results in NaN\n",
    "                # print(f\"Warning: NaN detected in embedding for sentence of '{occupation_name}'. Skipping sentence.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence for '{occupation_name}': {e}. Sentence: '{sentence[:100]}...'\")\n",
    "            continue # Skip this sentence, proceed to next\n",
    "\n",
    "    # Average the embeddings for the occupation\n",
    "    if sentence_embeddings:\n",
    "        averaged_embedding = np.mean(sentence_embeddings, axis=0)\n",
    "        return averaged_embedding\n",
    "    else:\n",
    "        print(f\"Warning: No valid sentence embeddings generated for occupation '{occupation_name}'. Returning None.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be3d20e",
   "metadata": {},
   "source": [
    "## 7. Generate Contextual Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ef20b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_embeddings = {}\n",
    "failed_occupations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c95953ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764ee2b8fbbd4f2e9c2df21169ba350b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Embeddings:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for occupation in tqdm(final_occupation_list, desc=\"Generating Embeddings\"):\n",
    "    occupation_sentences = final_sentences_dict.get(occupation, [])\n",
    "    avg_emb = get_contextual_embedding(occupation, occupation_sentences, tokenizer, model, DEVICE)\n",
    "\n",
    "    if avg_emb is not None:\n",
    "        contextual_embeddings[occupation] = avg_emb\n",
    "    else:\n",
    "        failed_occupations.append(occupation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d31fecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contextual embedding generation complete.\n",
      "Successfully generated embeddings for 26 occupations.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nContextual embedding generation complete.\")\n",
    "print(f\"Successfully generated embeddings for {len(contextual_embeddings)} occupations.\")\n",
    "if failed_occupations:\n",
    "    print(f\"Warning: Failed to generate embeddings for {len(failed_occupations)} occupations:\")\n",
    "    print(failed_occupations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f84f818e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Remove failed occupations from the final dataframe if any failed\n",
    "if failed_occupations:\n",
    "     print(\"Removing failed occupations from metadata dataframe.\")\n",
    "     df_final_occupations = df_final_occupations[~df_final_occupations['occupation'].isin(failed_occupations)].reset_index(drop=True)\n",
    "     print(f\"Metadata dataframe now contains {len(df_final_occupations)} occupations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f92a13",
   "metadata": {},
   "source": [
    "## 8. Save Metadata and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc0e4727",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving clustering metadata to: /Users/jessie/Documents/Projects/master_thesis_llms_bias/results/semantic_clustering/clustering_metadata.csv\n",
      "Metadata saved successfully.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving clustering metadata to: {CLUSTER_METADATA_OUTPUT_CSV}\")\n",
    "try:\n",
    "    # Save only the metadata for occupations we successfully embedded\n",
    "    df_final_occupations.to_csv(CLUSTER_METADATA_OUTPUT_CSV, index=False, encoding='utf-8')\n",
    "    print(\"Metadata saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving metadata: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ca1307b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving contextual embeddings to: /Users/jessie/Documents/Projects/master_thesis_llms_bias/results/semantic_clustering/clustering_contextual_embeddings.npz\n",
      "Embeddings saved successfully.\n",
      "Saved embeddings for 26 occupations.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving contextual embeddings to: {CLUSTER_EMBEDDINGS_OUTPUT_NPZ}\")\n",
    "try:\n",
    "    # Ensure embeddings dict only contains successfully processed occupations\n",
    "    final_embeddings_to_save = {occ: emb for occ, emb in contextual_embeddings.items() if occ in df_final_occupations['occupation'].values}\n",
    "    np.savez_compressed(CLUSTER_EMBEDDINGS_OUTPUT_NPZ, **final_embeddings_to_save)\n",
    "    print(\"Embeddings saved successfully.\")\n",
    "    print(f\"Saved embeddings for {len(final_embeddings_to_save)} occupations.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving embeddings: {e}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
