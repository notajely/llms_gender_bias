{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ba30d8",
   "metadata": {},
   "source": [
    "# Notebook 2: Embedding Generation using GPT-2\n",
    "\n",
    "**Objective:** Load the pre-trained GPT-2 model and tokenizer to generate static embeddings for:\n",
    "1. The 100 curated occupations (from the validated dictionary created in Notebook 1).\n",
    "2. The standard gender anchor terms ('he', 'she', 'man', 'woman').\n",
    "\n",
    "**Method:** We will use **Masked Mean Pooling** over the token embeddings from the last hidden layer of GPT-2 to obtain a single vector representation for each occupation/term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88818145",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a831fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter!\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8338c38",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d2af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "# Get project root assuming the notebook is in 'notebooks' directory\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b7c3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file (validated dictionary from Notebook 1)\n",
    "INPUT_CSV_FILE = project_root / 'results' / 'occupation_dictionary_validated.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84c91c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file for embeddings\n",
    "RESULTS_DIR = project_root / 'results'\n",
    "EMBEDDING_OUTPUT_FILE = RESULTS_DIR / 'gpt2_static_embeddings.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f5cf81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Configuration ---\n",
    "MODEL_NAME = 'gpt2'\n",
    "GENDER_TERMS = ['he', 'she', 'man', 'woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dddf5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if it doesn't exist (it should from Notebook 1, but check again)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2643d580",
   "metadata": {},
   "source": [
    "## 3. Setup Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d78b673",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, otherwise CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps') # For Apple Silicon GPUs\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20666e7e",
   "metadata": {},
   "source": [
    "## 4. Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4af2d7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading gpt2 tokenizer and model...\n",
      "Tokenizer lacks padding token. Setting EOS token as PAD token.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\nLoading {MODEL_NAME} tokenizer and model...\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = GPT2Model.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- Handle Padding Token ---\n",
    "# GPT-2 doesn't have a default PAD token, use EOS token instead\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Tokenizer lacks padding token. Setting EOS token as PAD token.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Important: Resize model embeddings to accommodate the change if PAD was truly added\n",
    "    # Although using EOS doesn't add a *new* token, explicitly setting it clarifies padding behavior.\n",
    "    # model.resize_token_embeddings(len(tokenizer)) # Usually not needed if just assigning existing EOS\n",
    "\n",
    "# Move model to the selected device\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Set model to evaluation mode (disables dropout, etc.) # Set model to evaluation mode (disables dropout, etc.)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7ce4c",
   "metadata": {},
   "source": [
    "## 5. Define Embedding Function (Masked Mean Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27c5caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Generates an embedding for the input text using masked mean pooling\n",
    "    over the last hidden state of the provided transformer model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text (word or phrase).\n",
    "        tokenizer: The loaded Hugging Face tokenizer.\n",
    "        model: The loaded Hugging Face model.\n",
    "        device: The torch device ('cuda', 'mps', or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray or None: The embedding vector as a NumPy array,\n",
    "                             or None if an error occurs or input is invalid.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        print(f\"Warning: Invalid input text provided: '{text}'. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Tokenize input - Ensure padding and truncation\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,        # Pad to the longest sequence in the batch (or max_length if specified)\n",
    "            truncation=True,     # Truncate sequences longer than model max length\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            return_attention_mask=True\n",
    "        ).to(device)\n",
    "\n",
    "        # Get model output without calculating gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract last hidden states (batch_size, sequence_length, hidden_size)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # --- Masked Mean Pooling ---\n",
    "        # Get attention mask (batch_size, sequence_length)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        # Expand mask dimensions to match hidden states: (batch_size, sequence_length, hidden_size)\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "        # Zero out padding tokens' embeddings\n",
    "        masked_embeddings = last_hidden_states * mask_expanded\n",
    "        # Sum embeddings across the sequence dimension (dim=1)\n",
    "        summed_embeddings = torch.sum(masked_embeddings, 1)\n",
    "        # Count actual (non-padding) tokens\n",
    "        # Sum the attention mask across sequence dimension (dim=1)\n",
    "        # Clamp ensures count is at least 1 to avoid division by zero\n",
    "        token_counts = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        # Calculate the mean\n",
    "        mean_pooled_embedding = summed_embeddings / token_counts\n",
    "        # --- End Masked Mean Pooling ---\n",
    "\n",
    "        # Move embedding to CPU and convert to NumPy array\n",
    "        # Squeeze removes the batch dimension (assumes batch size 1 here)\n",
    "        embedding_np = mean_pooled_embedding.squeeze().cpu().numpy()\n",
    "\n",
    "        # Check for NaNs in the final embedding\n",
    "        if np.isnan(embedding_np).any():\n",
    "             print(f\"Warning: NaN detected in embedding for '{text}'. Skipping.\")\n",
    "             return None\n",
    "\n",
    "        return embedding_np\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding for '{text}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9eae3e7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully generated test embedding for 'example' with shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# Test the function with a sample word\n",
    "test_word = \"example\"\n",
    "test_embedding = get_embedding(test_word, tokenizer, model, DEVICE)\n",
    "if test_embedding is not None:\n",
    "    print(f\"\\nSuccessfully generated test embedding for '{test_word}' with shape: {test_embedding.shape}\")\n",
    "else:\n",
    "    print(f\"\\nFailed to generate test embedding for '{test_word}'. Check function/model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c006de7",
   "metadata": {},
   "source": [
    "## 6. Load Occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe519c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading occupations from: /Users/jessie/Documents/Projects/master_thesis_llms_bias/results/occupation_dictionary_validated.csv\n",
      "Loaded 100 unique occupations.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading occupations from: {INPUT_CSV_FILE}\")\n",
    "try:\n",
    "    df_occupations = pd.read_csv(INPUT_CSV_FILE)\n",
    "    occupation_list = df_occupations['occupation'].dropna().unique().tolist()\n",
    "    print(f\"Loaded {len(occupation_list)} unique occupations.\")\n",
    "    if len(occupation_list) != 100:\n",
    "         print(f\"Warning: Expected 100 occupations based on paper, but found {len(occupation_list)} unique entries.\")\n",
    "except FileNotFoundError:\n",
    "     print(f\"Error: Validated dictionary file not found at {INPUT_CSV_FILE}\")\n",
    "     print(\"Please ensure Notebook 1 was run successfully and the file exists.\")\n",
    "     raise\n",
    "except KeyError:\n",
    "    print(f\"Error: Column 'occupation' not found in {INPUT_CSV_FILE}.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error reading occupation data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249d39c",
   "metadata": {},
   "source": [
    "## 7. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21f80d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c8a29981694bc2aed2aee241230c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gender Terms:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender term embeddings generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Embed Gender Terms ---\n",
    "embeddings = {}\n",
    "failed_terms = []\n",
    "for term in tqdm(GENDER_TERMS, desc=\"Gender Terms\"):\n",
    "    emb = get_embedding(term, tokenizer, model, DEVICE)\n",
    "    if emb is not None:\n",
    "        embeddings[term] = emb\n",
    "    else:\n",
    "        failed_terms.append(term)\n",
    "        print(f\"FATAL: Failed to get embedding for essential gender term '{term}'. Cannot proceed.\")\n",
    "        # Optional: Raise an error or exit if essential terms fail\n",
    "        # raise ValueError(f\"Embedding failed for gender term: {term}\")\n",
    "        \n",
    "if not all(term in embeddings for term in GENDER_TERMS):\n",
    "     raise ValueError(\"Failed to generate embeddings for one or more essential gender terms.\")\n",
    "else:\n",
    "    print(\"Gender term embeddings generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b65b8838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding occupations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2a419e4f034fef94f813efb90947d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Occupations:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding generation complete.\n",
      "Successfully generated embeddings for 104 terms.\n"
     ]
    }
   ],
   "source": [
    "# --- Embed Occupations ---\n",
    "print(\"\\nEmbedding occupations...\")\n",
    "for occupation in tqdm(occupation_list, desc=\"Occupations\"):\n",
    "    # Optional: Clean again just in case, though should be clean from Notebook 1\n",
    "    occupation_clean = occupation.strip()\n",
    "    if not occupation_clean:\n",
    "        failed_terms.append(f\"(Empty Occupation: Original '{occupation}')\")\n",
    "        continue\n",
    "\n",
    "    emb = get_embedding(occupation_clean, tokenizer, model, DEVICE)\n",
    "    if emb is not None:\n",
    "        embeddings[occupation] = emb # Use original name as key for consistency\n",
    "    else:\n",
    "        failed_terms.append(occupation)\n",
    "        \n",
    "print(f\"\\nEmbedding generation complete.\")\n",
    "print(f\"Successfully generated embeddings for {len(embeddings)} terms.\")\n",
    "if failed_terms:\n",
    "    print(f\"Warning: Failed to generate embeddings for {len(failed_terms)} terms:\")\n",
    "    # Print only the first few failed terms for brevity\n",
    "    print(failed_terms[:20]) # Adjust number as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894f53f",
   "metadata": {},
   "source": [
    "## 8. Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a93c20c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Use np.savez_compressed for potentially smaller file size\n",
    "    np.savez_compressed(EMBEDDING_OUTPUT_FILE, **embeddings)\n",
    "    print(\"Embeddings saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving embeddings: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6933ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
